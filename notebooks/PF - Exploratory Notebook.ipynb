{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.make(\n",
    "    env_name=\"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    "    use_object_obs=True,                    \n",
    "    horizon = 200, \n",
    "    reward_shaping=True                 \n",
    ")\n",
    "\n",
    "obs = env.reset()\n",
    "done = False       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pi = Variable(torch.FloatTensor([math.pi])).to(device)\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi).sqrt()\n",
    "    return a*b\n",
    "    \n",
    "\n",
    "class REINFORCEPolicy(nn.Module):\n",
    "    '''\n",
    "    This class represent our policy parameterization.\n",
    "    '''\n",
    "    def __init__(self, state_dim,action_dim,hidden_size):\n",
    "        super(REINFORCEPolicy, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_dim, self.hidden_size)\n",
    "        self.l2 = nn.Linear(self.hidden_size, self.hidden_size//2)\n",
    "        self.l3 = nn.Linear(self.hidden_size//2, self.action_dim)\n",
    "        self.l3_ = nn.Linear(self.hidden_size//2, self.action_dim)\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.d1(self.l1(x)))\n",
    "        out = F.relu(self.d2(self.l2(out)))\n",
    "        mu = self.l3(out)\n",
    "        sigma_sq = self.l3_(out)\n",
    "        return mu, sigma_sq\n",
    "    \n",
    "    \n",
    "class REINFORCE:\n",
    "    '''\n",
    "    This class encapsulates functionality required to run the REINFORCE algorithm.\n",
    "    '''\n",
    "    def __init__(self, state_dim,action_dim, gamma, lr, episodes, horizon, hidden_size):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.model = REINFORCEPolicy(state_dim, action_dim,hidden_size)\n",
    "        self.model = self.model.to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.lr)\n",
    "        self.model.train()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        \n",
    "    def select_action(self, state):\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        mu , sigma_sq = self.model(Variable(state).to(device)) \n",
    "        for i in range(self.action_dim):\n",
    "            mu_ = mu[i]\n",
    "            sigma_sq_ = sigma_sq[i]\n",
    "            sigma_sq_ = F.softplus(sigma_sq_) # ensures that the estimate is always positive\n",
    "\n",
    "            eps = torch.randn(mu_.size())\n",
    "            action = (mu_ + sigma_sq_.sqrt()*Variable(eps).to(device)).data\n",
    "            prob = normal(action, mu_, sigma_sq_)\n",
    "            log_prob = prob.log()\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "        \n",
    "        return actions, log_probs\n",
    "    \n",
    "\n",
    "    def episode_update_parameters(self, rewards, log_probs):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = torch.zeros(self.action_dim)\n",
    "        for i in reversed(range(self.horizon)):\n",
    "            R = self.gamma * R + rewards[0][i]\n",
    "            for j in range(self.action_dim):\n",
    "                loss[j] = loss[j] - (log_probs[0][i][j]*(Variable(R.data.squeeze()).expand_as(log_probs[0][i][j])).to(device)).sum()\n",
    "        loss = loss.sum()\n",
    "        loss = loss / len(rewards)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def epoch_update_parameters(self, rewards, log_probs):\n",
    "        R = torch.zeros(self.episodes)\n",
    "        loss = torch.zeros(self.episodes,self.action_dim)\n",
    "        for episode in range(self.episodes):\n",
    "            for i in reversed(range(self.horizon)):\n",
    "                R[episode] = self.gamma * R[episode] + rewards[episode][i]\n",
    "                for j in range(self.action_dim):\n",
    "                    loss[episode][j] = loss[episode][j] - (log_probs[episode][i][j]*(Variable(R[episode].data.squeeze()).expand_as(log_probs[episode][i][j])).to(device)).sum()\n",
    "        \n",
    "        loss = loss.sum(dim=0)/self.episodes\n",
    "        loss = loss.sum()\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.REINFORCE import REINFORCE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = suite.make(\n",
    "    env_name=\"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    "    use_object_obs=True,                    \n",
    "    horizon = 150, \n",
    "    reward_shaping=True                 \n",
    ")\n",
    "obs = env.reset()\n",
    "state_dim = obs['robot0_robot-state'].shape[0]+obs['object-state'].shape[0]\n",
    "\n",
    "\n",
    "agent = REINFORCE(state_dim,env.action_dim,0.9,0.001,100,200,256)\n",
    "agent.model.load_state_dict(torch.load('/Users/peterfagan/Downloads/REINFORCE_3.pkl'))\n",
    "\n",
    "obs=env.reset()\n",
    "state = torch.Tensor(np.append(obs['robot0_robot-state'],obs['object-state']))\n",
    "done=False\n",
    "while done==False: \n",
    "    action, log_prob = agent.select_action(state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing learnt behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.make(\n",
    "    env_name=\"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    "    use_object_obs=True,                    \n",
    "    horizon = 200, \n",
    "    reward_shaping=True                 \n",
    ")\n",
    "obs=env.reset()\n",
    "state = torch.Tensor(np.append(obs['robot0_robot-state'],obs['object-state']))\n",
    "done=False\n",
    "log_probs = []\n",
    "rewards = []\n",
    "while done==False: \n",
    "    action, log_prob = agent.select_action(state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    log_probs.append(log_prob)\n",
    "    rewards.append(reward)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "# Implementation of DDPG algorithm with inspiration from\n",
    "# \"https://github.com/ghliu/pytorch-ddpg/blob/master/ddpg.py\"\n",
    "\n",
    "import robosuite as suite\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from models.utils import * # Improve by adding path var\n",
    "\n",
    "\n",
    "class DDPGActor(nn.Module):\n",
    "    '''This class represents our actor model'''\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    '''This class represents our critic model'''\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim+action_dim, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x, a = xs\n",
    "        x = F.relu(self.l1(torch.cat([x, a], 1)))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    '''This class represents our implementation of DDPG'''\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, args):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.actor = DDPGActor(state_dim, action_dim, args.hidden_size)\n",
    "        self.actor = self.actor.to(device)\n",
    "        self.actor_target = DDPGActor(state_dim, action_dim, args.hidden_size)\n",
    "        self.actor_target = self.actor_target.to(device)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=args.lr)\n",
    "\n",
    "        self.critic = DDPGCritic(state_dim, action_dim, args.hidden_size)\n",
    "        self.critic = self.critic.to(device)\n",
    "        self.critic_target = DDPGCritic(\n",
    "            state_dim, action_dim, args.hidden_size)\n",
    "        self.critic_target = self.critic_target.to(device)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=args.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        hard_update(self.actor_target, self.actor)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        self.max_mem_size = args.max_mem_size\n",
    "        self.memory = ReplayBuffer(args.max_mem_size, state_dim, action_dim)\n",
    "\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(args.theta)\n",
    "\n",
    "        self.tau = args.tau\n",
    "        self.batch_size = args.batch_size\n",
    "        self.lr = args.lr\n",
    "        self.gamma = args.gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.depsilon = 1.0 / args.epsilon\n",
    "\n",
    "        self.s_t = None\n",
    "        self.a_t = None\n",
    "        self.is_training = True\n",
    "\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.store_transition(self.s_t, self.a_t, r_t, s_t1, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "\n",
    "    def select_action(self, state, decay_epsilon=True):\n",
    "        action = self.actor(to_tensor(state)).detach().numpy()\n",
    "        action += self.is_training*self.random_process.sample()\n",
    "        action = np.clip(action, -1., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "        \n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1.,1.,self.action_dim)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def update_parameters(self):\n",
    "        # Sample batch from replay buffer\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, done_batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Calculate next q-values\n",
    "        q_next = self.critic_target([to_tensor(next_state_batch), \\\n",
    "                     self.actor_target(to_tensor(next_state_batch))])\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.gamma*to_tensor(done_batch)*q_next\n",
    "\n",
    "        # Critic update\n",
    "        self.critic.zero_grad()\n",
    "\n",
    "        q_batch = self.critic([to_tensor(state_batch), to_tensor(action_batch)])\n",
    "        value_loss = self.criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update \n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            self.actor(to_tensor(state_batch))\n",
    "        ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Target update\n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "        \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    hidden_size = 256\n",
    "    max_mem_size=2000\n",
    "    tau=0.001\n",
    "    batch_size=5\n",
    "    lr=0.001\n",
    "    epsilon=10000\n",
    "    warmup=10\n",
    "    gamma=0.99\n",
    "    theta=0.15\n",
    "    num_episodes=50\n",
    "    horizon=10\n",
    "    env_name='Lift'\n",
    "    robot='Panda'\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.make(\n",
    "        env_name=args.env_name,\n",
    "        robots=args.robot,\n",
    "        has_renderer=False,\n",
    "        has_offscreen_renderer=False,\n",
    "        use_camera_obs=False,\n",
    "        use_object_obs=True,                    \n",
    "        horizon = args.horizon, \n",
    "        reward_shaping=True                 \n",
    "    )\n",
    "obs = env.reset()\n",
    "state_dim = obs['robot0_robot-state'].shape[0]+obs['object-state'].shape[0]\n",
    "agent = DDPG(state_dim, env.action_dim, args)\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    state = np.append(obs['robot0_robot-state'],obs['object-state'])\n",
    "    agent.s_t = state\n",
    "    done=False\n",
    "    while done==False: \n",
    "        action = agent.random_action()\n",
    "        obs, reward,done, info = env.step(action)\n",
    "        state = np.append(obs['robot0_robot-state'],obs['object-state'])\n",
    "        agent.observe(reward, state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, state_, done = agent.memory.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    q_next = agent.critic_target([to_tensor(state_), \\\n",
    "                         agent.actor_target(to_tensor(state_)).detach()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0736],\n",
       "        [-0.0790],\n",
       "        [-0.0710],\n",
       "        [-0.0585],\n",
       "        [-0.0669],\n",
       "        [-0.0787],\n",
       "        [-0.0138],\n",
       "        [-0.0441],\n",
       "        [-0.0813],\n",
       "        [-0.0524]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0664],\n",
       "        [-0.0699],\n",
       "        [-0.0668],\n",
       "        [-0.0532],\n",
       "        [-0.0643],\n",
       "        [-0.0726],\n",
       "        [-0.0128],\n",
       "        [-0.0425],\n",
       "        [-0.0730],\n",
       "        [-0.0439]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_q = to_tensor(reward) + agent.gamma*q_next \n",
    "target_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic.zero_grad()\n",
    "q = agent.critic([to_tensor(state),to_tensor(action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = agent.criterion(q, target_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = suite.make(\n",
    "        env_name=args.env_name,\n",
    "        robots=args.robot,\n",
    "        has_renderer=False,\n",
    "        has_offscreen_renderer=False,\n",
    "        use_camera_obs=False,\n",
    "        use_object_obs=True,                    \n",
    "        horizon = args.horizon, \n",
    "        reward_shaping=True                 \n",
    "    )\n",
    "obs = env.reset()\n",
    "state_dim = obs['robot0_robot-state'].shape[0]+obs['object-state'].shape[0]\n",
    "\n",
    "agent = DDPG(state_dim, env.action_dim, args)\n",
    "iteration = 0\n",
    "for episode in range(args.num_episodes):\n",
    "    obs = env.reset()\n",
    "    state = np.append(obs['robot0_robot-state'],obs['object-state'])\n",
    "    agent.s_t = state\n",
    "    done=False\n",
    "    while done==False: \n",
    "        if iteration <= args.warmup:\n",
    "            action = agent.random_action()\n",
    "            iteration += 1\n",
    "        else:\n",
    "            action = agent.select_action(state) \n",
    "            iteration += 1\n",
    "        obs, reward,done, info = env.step(action)\n",
    "        state = np.append(obs['robot0_robot-state'],obs['object-state'])\n",
    "        agent.observe(reward, state, done)\n",
    "        if iteration > args.warmup:\n",
    "            agent.update_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
