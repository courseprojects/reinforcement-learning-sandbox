{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('robot0_joint_pos',\n",
       "              array([-0.0219546 ,  0.20314689,  0.02159294, -2.60401192,  0.03396985,\n",
       "                      2.9417042 ,  0.76500556])),\n",
       "             ('robot0_joint_vel',\n",
       "              array([-3.84357389e-10, -1.99050302e-01, -1.15345653e-06, -6.67097603e-01,\n",
       "                     -2.34813793e-03,  1.71713143e+00,  4.25741437e-04])),\n",
       "             ('robot0_eef_pos',\n",
       "              array([-9.69841837e-02, -9.16511673e-04,  1.01336326e+00])),\n",
       "             ('robot0_eef_quat',\n",
       "              array([ 0.99769611, -0.00651168,  0.06726765, -0.0059277 ])),\n",
       "             ('robot0_gripper_qpos', array([ 0.020833, -0.020833])),\n",
       "             ('robot0_gripper_qvel', array([-0.08876024, -0.10134752])),\n",
       "             ('robot0_robot-state',\n",
       "              array([-2.19528369e-02,  2.01752500e-01,  2.15912571e-02, -5.12059464e-01,\n",
       "                      3.39633156e-02,  1.98560011e-01,  6.92541010e-01,  9.99759007e-01,\n",
       "                      9.79436536e-01,  9.99766882e-01, -8.58950002e-01,  9.99423080e-01,\n",
       "                     -9.80088732e-01,  7.21378506e-01, -3.84357389e-10, -1.99050302e-01,\n",
       "                     -1.15345653e-06, -6.67097603e-01, -2.34813793e-03,  1.71713143e+00,\n",
       "                      4.25741437e-04, -9.69841837e-02, -9.16511673e-04,  1.01336326e+00,\n",
       "                      9.97696108e-01, -6.51167538e-03,  6.72676499e-02, -5.92770105e-03,\n",
       "                      2.08330000e-02, -2.08330000e-02, -8.87602417e-02, -1.01347520e-01])),\n",
       "             ('cube_pos', array([ 0.00771401, -0.01225266,  0.83129523])),\n",
       "             ('cube_quat',\n",
       "              array([0.        , 0.        , 0.91133468, 0.41166626])),\n",
       "             ('robot0_gripper_to_cube',\n",
       "              array([-0.10469819,  0.01133615,  0.18206803])),\n",
       "             ('object-state',\n",
       "              array([ 0.00771401, -0.01225266,  0.83129523,  0.        ,  0.        ,\n",
       "                      0.91133468,  0.41166626, -0.10469819,  0.01133615,  0.18206803]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = suite.make(\n",
    "    env_name=\"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    "    use_object_obs=True,                    \n",
    "    horizon = 200, \n",
    "    reward_shaping=True                 \n",
    ")\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist={}\n",
    "for i in range(200):\n",
    "    action = np.random.randn(env.robots[0].dof) # sample random action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    hist['step_{}'.format(i+1)] = np.append(obs['robot0_robot-state'], obs['object-state'])   # take action in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.randn(env.robots[0].dof) # sample random action\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('robot0_joint_pos',\n",
       "              array([-0.0302742 ,  0.14044944, -0.00535421, -2.68449143,  0.00339097,\n",
       "                      3.10627978,  0.82223303])),\n",
       "             ('robot0_joint_vel',\n",
       "              array([ 0.10565759, -0.27393351,  0.05012956, -0.6054687 ,  0.03996287,\n",
       "                      1.47560382,  0.25854712])),\n",
       "             ('robot0_eef_pos',\n",
       "              array([-0.09538131, -0.01710642,  1.0367943 ])),\n",
       "             ('robot0_eef_quat',\n",
       "              array([ 9.89587659e-01, -3.70745152e-02,  1.39074572e-01,  9.09663361e-05])),\n",
       "             ('robot0_gripper_qpos', array([ 0.00972155, -0.00984648])),\n",
       "             ('robot0_gripper_qvel', array([-0.18717638,  0.16809092])),\n",
       "             ('robot0_robot-state',\n",
       "              array([-3.02695780e-02,  1.39988145e-01, -5.35418202e-03, -4.41348789e-01,\n",
       "                      3.39096588e-03,  3.53055377e-02,  7.32667427e-01,  9.99541771e-01,\n",
       "                      9.90153180e-01,  9.99985666e-01, -8.97335638e-01,  9.99994251e-01,\n",
       "                     -9.99376565e-01,  6.80586837e-01,  1.05657593e-01, -2.73933512e-01,\n",
       "                      5.01295630e-02, -6.05468698e-01,  3.99628690e-02,  1.47560382e+00,\n",
       "                      2.58547124e-01, -9.53813072e-02, -1.71064171e-02,  1.03679430e+00,\n",
       "                      9.89587659e-01, -3.70745152e-02,  1.39074572e-01,  9.09663361e-05,\n",
       "                      9.72155472e-03, -9.84648310e-03, -1.87176380e-01,  1.68090916e-01])),\n",
       "             ('cube_pos', array([-0.02960184,  0.02625174,  0.81797635])),\n",
       "             ('cube_quat',\n",
       "              array([ 0.        ,  0.        ,  0.88070973, -0.47365638])),\n",
       "             ('robot0_gripper_to_cube',\n",
       "              array([-0.06577946, -0.04335815,  0.21881796])),\n",
       "             ('object-state',\n",
       "              array([-0.02960184,  0.02625174,  0.81797635,  0.        ,  0.        ,\n",
       "                      0.88070973, -0.47365638, -0.06577946, -0.04335815,  0.21881796]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceModel(nn.Module):\n",
    "    '''\n",
    "    This class represent our policy parameterization\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ReinforceModel, self).__init__()\n",
    "        self.robot_dim = len(env.observation_spec()['robot0_robot-state'])\n",
    "        self.object_dim = len(env.observation_spec()['object-state'])\n",
    "        self.state_dim = self.robot_dim + self.object_dim\n",
    "        self.action_dim = env.action_dim\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_dim, 128, bias = False)\n",
    "        self.l2 = nn.Linear(128, self.action_dim, bias = False)\n",
    "        \n",
    "        self.gamma = gamma # discount\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def forward(self,x):\n",
    "        model = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "         )\n",
    "        return model(x)\n",
    "    \n",
    "# Sepcify hyper parameters in a config file    \n",
    "gamma = 0.9\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "policy = ReinforceModel()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''This function outputs the required actlion torques'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    '''This function updates our current policy parameterization'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    '''This function trains the network'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
